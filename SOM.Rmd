---
title: "Self-Organizing Maps"
author: "Inayatus"
date: "`r format(Sys.Date(), '%A, %B-%d-%Y')`"
output:
  rmdformats::readthedown:
    
    self_contained : yes
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: default
---

```{r setup, include=FALSE}
library(rmdformats)
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
```

# Introduction Self-Organizing Maps (SOM)
Self-Organizing Maps first introduce by Teuvo Kohonen. According to the `Wiki`, Self-Organizing Map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction.[^1] SOM are an unsupervised data visualisation technique that can be used to visualise high-dimensional data sets in lower (typically 2) dimensional representations.[^2] SOM also represent clustering concept by grouping the similar features together. So, SOM can use to cluster high-dimensional data sets by doing dimensionality reduction and visualize it using maps.

<center> ![](arsitektur.PNG) </center>

Each data point in the data set recognizes themselves by competeting for representation. SOM mapping steps starts from initializing the weight vectors. From there a sample vector is selected randomly and the map of weight vectors is searched to find which weight best represents that sample. Each weight vector has neighborhood weights that are close to it. The weight that is chosen is rewarded by being able to become more like that randomly selected sample vector. The neighbors of that weight are also rewarded by being able to become more like the chosen sample vector. This allows the map to grow and form different shapes. Most generally, they form square/rectangular/hexagonal/L shapes in 2D feature space.

SOM calculate the distance of each input vector by each weight of nodes. Distance that usually used is Euclidean distance.  

*Algoritma SOM :*

 - Preparing input data for training data sets.
 - Each node’s weights are initialized. A vector is chosen at random from the set of training data.
 - Calculate the distance of each node. The winning node is commonly known as the Best Matching Unit (BMU).
 - Updating of weight, then calculate again which one's weights are most like the input vectors. 
 - Every BMU has a neighbour then calculated. The amout of neighbour is decrease over time. 
 - The winning weight is rewarded with becoming more like the sample vector. The nighbors also become more like the sample vector. The closer a node is to the BMU, the more its weights get altered and the farther away the neighbor is from the BMU, the less it learns.
 - Repeat the 2-5 step until every node getting close to input vectors. 
 
 <center> ![](som-training.png) </center>
 
*Implementation use case for SOM algorithm :*

1. Customer segmentation
2. Fantasy league analysis
3. Cyber profilling criminals
4. Identifying crime
5. Etc


# SOM Analysis

On this article, we want to use `web analytics` dataset in San Francisco. We want to learn *how to use SOM to identify characteristics of each cluster* in many web on San Francisco by some features.

## Library Used
On this article, we used `kohonen` package for making a SOM algorithm. Since we know that `kohonen` packages are from `class`, we use it to make many key function such as :

 - *somgrid()* : initialized SOM node set
 - *som()* : making a SOM model, change the radius of neighbourhood, learning rate, and iteration
 - *plot.kohonen()* / *plot()* : visualisation of resulting SOM
 
```{r, message=FALSE}
library(kohonen)
```

In this article, we want to use `ads data` from xyz company that make an ads on Facebook. 

## Data Pre-Processing
```{r}
ads <- read.csv("data_input/KAG_conversion_data.csv") %>% 
  glimpse()

ads <- ads %>% 
  mutate(ad_id = as.factor(ad_id),
         xyz_campaign_id = as.factor(xyz_campaign_id),
         fb_campaign_id = as.factor(fb_campaign_id)) %>% 
  glimpse()

levels(ads$xyz_campaign_id)
```
 - `ad_id` : an unique ID of each ad
 - `xyz_campaign_id` : an ID associated with each ad campaign of XYZ company
 - `fb_campaign_id` : an ID associated with how Facebook tracks each campaign
 - `age` : age of the person to whom the ad is shown
 - `gender` : gender of the person to whon thw ad si shown
 - `interest` : a code specifying the category to which the person’s interest belongs (interests are as mentioned in the person’s Facebook public profile)
 - `Impressions` : number of time the ad is shown
 - `Clicks` : number od click on for the ad
 - `Spent` : amount paid from xyz company to Facebook to shown the ad
 - `Total_Conversion` : total number of people who enquired about the product after seeing the ad
 - `approved_Conversion` : total number of people who bougth the product after seeing the ad

As we know that if SOM used numerical data, so if we have chategorical variables we must to change those variable to dummy variables.
```{r}
# change the chategoric variables to a dummy variables
ads.s <- ads %>% 
  mutate(genderM = ifelse(gender == "M", 1, 0),
         age2 = ifelse(age == "35-39", 1, 0),
         age3 = ifelse(age == "40-44", 1, 0),
         age4 = ifelse(age == "45-49", 1, 0)) %>% 
  select(-c(1,3:5))

# make a train data sets that scaled and convert them to be a matrix cause kohonen function accept numeric matrix
ads.train <- as.matrix(scale(ads.s[,-1]))
```


```{r}
# make a SOM grid
set.seed(100)
ads.grid <- somgrid(xdim = 10, ydim = 10, topo = "hexagonal")

# make a SOM model
set.seed(100)
ads.model <- som(ads.train, ads.grid, rlen = 500, radius = 2.5, keep.data = TRUE,
                  dist.fcts = "euclidean")
# str(ads.model)
```
From that summary of `ads.model` we know that our som grid has 10x10 dimension. 

# Unsupervised SOM
## Visualize of SOM {.tabset .tabset-fade .tabset-pills}

Before visualize SOM model, let's explore list of SOM model. If we want to know that our data position in maps, we look it in `unit.classif`. Each values represent the node number to which this application belong. For example, in first value of application, 12 means that in first application has been claaified into 12 particular nodes. 
```{r, eval=FALSE}
ads.model$unit.classif
```

And we can see the classification of each nodes by codes plot and we can see the values of each nodes using the mapping plot.

```{r, fig.width=10}
plot(ads.model, type = "mapping", pchs = 19, shape = "round")
```


```{r}
head(data.frame(ads.train), 20)
```

```{r, fig.width=20}
plot(ads.model, type = "codes", main = "Codes Plot", palette.name = rainbow)
```

At the first node, we can say that the input data entered on the first node is characterized from the major variable tat have gender M and age range of 30-34. For the second node there is characterized of major variables that have gender M and age range 30-34. Here we can conclude that the node will have neighbors that have similar characteristics to it, that is, like nodes 1 and 2 that are close together because they have gender M and an age range of 30-34 that same.  

### Training Progress

As the SOM training iterations progress, the distance from each node’s weights to the samples represented by that node is reduced. Ideally, this distance should reach a minimum plateau. This plot option shows the progress over time. If the curve is continually decreasing, more iterations are required.
```{r}
plot(ads.model, type = "changes")
```

### Node Counts

The Kohonen packages allows us to visualise the count of how many samples are mapped to each node on the map. This metric can be used as a measure of map quality – ideally the sample distribution is relatively uniform. Large values in some map areas suggests that a larger map would be benificial. Empty nodes indicate that your map size is too big for the number of samples. Aim for at least 5-10 samples per node when choosing map size.

```{r, fig.width=20}
plot(ads.model, type = "counts")
```

Nodes that colored by red mean nodes that have the least number of input values, such as in the second nodes. If the color nodes are brighter indicating the nodes have a lot of input values, such as in the 31 nodes. For nodes that have gray colors, that means the nodes do not have input values at all.

### Neighbours Nodes

If we want to see nodes that have the closest or farthest neighbours, we can plot a plot based on `dist.neighbours`. Nodes that have darker colors mean that the nodes have a vector input that is closer, whereas nodes that have lighter colors mean that the nodes have vector inputs that are farther apart.

```{r, fig.width=20}
plot(ads.model, type = "dist.neighbours")
```


### Heatmaps

Heatmaps are perhaps the most important visualisation possible for Self-Organising Maps.  A SOM heatmap allows the visualisation of the distribution of a single variable across the map. Typically, a SOM investigative process involves the creation of multiple heatmaps, and then the comparison of these heatmaps to identify interesting areas on the map. It is important to remember that the individual sample positions do not move from one visualisation to another, the map is simply coloured by different variables.
The default Kohonen heatmap is created by using the type “heatmap”, and then providing one of the variables from the set of node weights. In this case we visualise the average education level on the SOM.
```{r, fig.width=20}
heatmap.som <- function(model){
  for (i in 1:10) {
   plot(model, type = "property", property = getCodes(model)[,i], 
        main = colnames(getCodes(model))[i]) 
  }
}
heatmap.som(ads.model)
```

From the heatmap that is formed, we can know which nodes have the characteristics of each variable whose value is high and the value is low.

### Clustering 

Then we want to try to clustering every observation that we have without looking the labels of `xyz_campaign_id`. Like making cluster using k-means, we must determine the number of cluster that we want to make. In this case, we want to use elbow method to determine number of cluster.


```{r, fig.width=20, message=FALSE}
library(factoextra)
set.seed(100)
fviz_nbclust(ads.model$codes[[1]], kmeans, method = "wss")
set.seed(100)
clust <- kmeans(ads.model$codes[[1]], 6)

# clustering using hierarchial
# cluster.som <- cutree(hclust(dist(ads.model$codes[[1]])), 6)
```


```{r, fig.width=20}
plot(ads.model, type = "codes", bgcol = rainbow(9)[clust$cluster], main = "Cluster Map")
add.cluster.boundaries(ads.model, clust$cluster)
```
At this part, to do the profiling of the clusters that we made, we do not need to look at the descriptive results of the cluster or look deeper into the data, we only need to look at the cluster map above.

The characteristics for each cluster :

 - Cluster 1 (red) : gender Male, age 30-34
 - Cluster 2 (orange) : gender Female, gender Male, age 40-44, interest middle
 - Cluster 3 (green light) : gender Female, gender Male,  age 45-49, interest high, impression middle, clicks middle
 - Cluster 4 (green) : interest high, impression high, clicks high, spent high, gender Male, age 30-34, total conversion high, approved conversion high
 - Cluster 5 (turqois) : gender Male, age 45-49, interest high, clicks and spent low
 - Cluster 6 (blue) : gender Female, age 45-49, interest, impression, clicks, spent, total_conversion low.

```{r}
# know cluster each data
ads.cluster <- data.frame(ads.s, cluster = clust$cluster[ads.model$unit.classif])
tail(ads.cluster, 10)

```


# Supervised SOM
In supervised SOM, we want to classify our SOM model with our dependent variable (xyz_campaign_id). 
```{r}
# split data
set.seed(100)
int <- sample(nrow(ads.s), nrow(ads.s)*0.8)
train <- ads.s[int,]
test <- ads.s[-int,]

# scaling data
trainX <- scale(train[,-1])
testX <- scale(test[,-1], center = attr(trainX, "scaled:center"))


# make label
train.label <- factor(train[,1])
test.label <- factor(test[,1])
test[,1] <- 916
testXY <- list(independent = testX, dependent = test.label)
```
## Classification
```{r}
# classification & predict
set.seed(100)
class <- xyf(trainX, classvec2classmat(train.label), ads.grid, rlen = 500)
```


```{r}
plot(class, type = "changes")
```
## Predict
```{r}
pred <- predict(class, newdata = testXY)
table(Predict = pred$predictions[[2]], Actual = test.label)
```
## Cluster Boundaries
```{r, fig.width=20}
plot(ads.model, type = "codes", bgcol = rainbow(9)[clust$cluster], main = "Cluster SOM")
add.cluster.boundaries(ads.model, clust$cluster)
```


```{r, fig.width=20}
c.class <- kmeans(class$codes[[2]], 3)
par(mfrow = c(1,2))
plot(class, type = "codes", main = c("Unsupervised SOM", "Supervised SOM"), 
     bgcol = rainbow(3)[c.class$cluster])
add.cluster.boundaries(class, c.class$cluster)
```

From the results of SOM clustering using unsupervised and supervised, we can conclude that for xyz_campaign_id 916 more interested in the ads displayed are Male and Female aged 30-49 and they are interested in existing ads and then make payments after seeing the ads.
For xyz_campaign_id 936, those who see the ads is Male and Female in ranging age from 30-39 and they are only interested in the ads that are displayed but to make payments after seeing the ads are small.
For xyz_campaign_id 1178, which saw more ads for Female and aged 30-34 years.


*Advantage using SOM :*

 - Intuitive method to develop profilling
 - Easy to explain the result with the maps
 - Can visualize high-dimensional data by two-dimensional maps
 
*Disadvantage using SOM :*

 - Requires clean, numeric data

## Learning by Building

Make a clustering SOM using `Web_Analytics.csv` datasets. You can make clustering using unsupervised method or supervised method. If using unsupervised method, to determine k please use Elbow method using kmeans. Make an insight that you can get from your work.

Happy trying!
 
# Annotation

[^1] [Wikipedia. Self-organizing Maps.](https://en.wikipedia.org/wiki/Self-organizing_map)

[^2] [Ralhan, A. 2018. _Self Organizing Maps_.](https://towardsdatascience.com/self-organizing-maps-ff5853a118d4)

[^3] [Lynn, Shane. Self-Organizing Maps for Customer Segementation using R.](https://www.shanelynn.ie/self-organising-maps-for-customer-segmentation-using-r/)

[^4] [Self Organizing Maps](https://www.superdatascience.com/blogs/self-organizing-maps-soms-how-do-self-organizing-maps-learn-part-1/)

[^5] [The Ultimate Guide to Self Organizing Maps (SOMs)](https://www.superdatascience.com/blogs/the-ultimate-guide-to-self-organizing-maps-soms)
 
 
